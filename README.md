# 词法分析器的设计与实现

## 设计要求

### 核心要求
设计一个程序，根据选定的源语言的构词规则，从输入文件中识别出该语言所有的合法的单词符号，并以等长的二元组形式输出。程序应该对无法识别的子串做相应处理。程序应将所有结果信息写入输出文件。

### I/O要求
**输入:** 一个纯文本文件，内容是字符串形式的源程序。  
**输出:** 一个纯文本文件，内容是由等长二元组形式的单词符号所构成的串（流）。

### 架构要求
该程序应该设计为至少包含2个模块：驱动模块和工作模块。  
- 驱动模块包含了程序的入口和出口，主要负责输入、输出处理并调用工作模块。  
- 工作模块负责可能需要有的清洗、分割、识别、编码等工作。

### 其他约束
- 测试用例：程序应该经过充分的测试，报告中要求列出至少使用5组测试用例进行测试的情况。
- 报告应包含对源语言的描述，例如：形式化描述和自然语言描述等。
- 报告应包含对词法规则的描述和由此展开的分析和设计，包括：构词规则，单词的分类和编码，单词的数据结构的设计，可能会有的符号表的设计、错误信息的设计等。
- 建议报告中包含对程序的主要流程、关键步骤的算法和总体结构的简要描述。
- 报告应包含程序正常运行的画面截图和使用测试用例测试程序的画面截图及文字说明。
- 应将程序源代码、测试用例及可能需要的资源文件打包作为报告附件。

## 设计思路

### 源语言描述
本实验的源语言为类C子集，包含以下基本词法单元：

类别|特征|正则式表达
:-:|:-:|:-:
关键字|一个特定的字符串|"if"\|\|"else"\|\|"while"\|\|"for"\|\|"int"\|\|"float"\|\|"return"\|\|"void"
标识符|以字母或下划线开头，后接字母、数字或下划线|[a-zA-Z_][a-zA-Z0-9_]*
数值常量|整数：由数字组成的序列；浮点数：包含小数点|("-"\|\|NULL)[1-9][0-9]\* ((.[0-9]\*) \|\| NULL)
运算符与界符|特定字符（可能为两字符）|"+" \| "-" \| "*" \| "/" \| "=" \| "==" \| ";" \| "," \| "(" \| ")"
注释|行注释以 // 开头至行尾；块注释以 /* 开始，*/ 结束

### 词法规则与设计

#### 单词分类与编码
类别|编码|示例
-|-|-
关键字|1|if, int
标识符|2|x, sum
整数|3|42
浮点数|4|3.14
运算符|5|+, ==
界符|6|;, (
错误字符|7|@, #

#### 数据结构设计
Token结构体
存储单词类型和词素：
```cpp
struct Token {
    TokenType type;   // 类型编码
    std::string lexeme; // 词素字符串
};
```

符号表
使用哈希表预存关键字和运算符，快速查找：
```cpp
const std::unordered_set<std::string> KEYWORDS = {"if", "else", ...};
const std::unordered_set<std::string> OPERATORS = {"+", "==", ...};
```

### 程序设计与实现
#### 模块划分
- **驱动模块（main.cpp）**  
负责文件输入/输出  
调用工作模块进行词法分析

- **工作模块（tokenizer.cpp）**  
清洗源程序（跳过空白、处理注释）  
分割并识别单词符号  
生成Token序列  

#### 源代码结构
```
lex_analyzer/
├── include/         // 头文件
├── src/             // 源文件
    ├──main.cpp
    └──tokenizer.cpp
├── I&O/             // 测试用例及输出结果
    ├──TestCases/
    └──output/
└── Makefile         // 编译脚本
```
#### 核心算法流程
- 初始化：读取源文件内容至字符串
- 逐字符扫描：  
**跳过空白符：** 空格、换行、制表符  
**处理注释：** 根据 // 或 /- 跳过后续内容

- 识别单词：  
**标识符/关键字：** 首字符为字母或下划线，后续字符为字母/数字/下划线  
**数值常量：** 数字开头，可能包含小数点  
**运算符：** 需检查最长匹配（如 == 优先于 =）  
**界符：** 单字符直接匹配  
**错误处理：** 无法识别的字符生成错误Token  

流程图：

```
开始
  ↓
读取源文件 → 失败 → 报错退出
  ↓
初始化扫描位置pos=0
  ↓
循环处理字符：
  跳过空白符 → 处理注释 → 识别单词 → 生成Token
  ↓
直到文件结束
  ↓
输出Token序列到文件
  ↓
结束
```
#### 关键代码片段
数值识别逻辑：
[handleNumber](src/tokenizer.cpp)
```cpp
Token Tokenizer::handleNumber(size_t& pos, const std::string& source) {
    bool has_dot = false;
    while (pos < source.size()) {
        if (isdigit(source[pos])) pos++;
        else if (source[pos] == '.' && !has_dot) { 
            has_dot = true; 
            pos++; 
        } else break;
    }
    // 检查是否为合法浮点数
    if (非法格式) return {TokenType::ERROR, lexeme};
}
```

### 测试用例与结果
#### 测试用例设计

#### 测试结果截图

### 优化方案
- 通过队列实现缓冲区
